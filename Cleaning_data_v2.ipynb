{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"sms-spam-collection-dataset/spam.csv\", encoding='latin-1')\n",
    "data_labels = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['v1', 'v2']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "documents = []\n",
    "while i < int(data.shape[0]):\n",
    "    documents.append((data.iloc[i][0], data.iloc[i][1].split()))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffling as to not get bias data\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Function that turns spam-ham dataset to 0-1 table\n",
    "\n",
    "labels = data_labels[['v1']]\n",
    "span_ham_list = []\n",
    "for i in labels.values:\n",
    "    span_ham_list.extend(i)\n",
    "    \n",
    "def spam_ham_converter(lst):\n",
    "    new_a = np.array([])\n",
    "    for i in lst:\n",
    "        if i == 'ham':\n",
    "            new_a = np.concatenate((new_a, np.array([0])))\n",
    "            continue\n",
    "        new_a = np.concatenate((new_a, np.array([1])))\n",
    "    return new_a.reshape(5572,1)\n",
    "\n",
    "spam_ham_list = spam_ham_converter(span_ham_list)\n",
    "Y = spam_ham_list\n",
    "Y            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "every_word = []\n",
    "for j in data[\"v2\"]:\n",
    "    j = j.lower()\n",
    "    split_word= j.split()\n",
    "    every_word.extend(split_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency distribution in NLTK to remove useless words\n",
    "every_word=nltk.FreqDist(every_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = [x[0] for x in every_word.most_common(1500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\""
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_features(d):\n",
    "    array = np.array([w in d for w in word_features])\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_set = np.array([])\n",
    "for m in data[\"v2\"]:\n",
    "    features_set = np.append(features_set, find_features(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_set = features_set.reshape(5572, 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rafael Martinez\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Rafael Martinez\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.003789389708815317"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##TRAINING TIME\n",
    "\n",
    "##Separate Data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features_set,Y,test_size=0.10)\n",
    "\n",
    "lg = LogisticRegression()\n",
    "lg.fit(X_train, Y_train)\n",
    "Y_predicted = lg.predict(X_train)\n",
    "Y_predicted\n",
    "\n",
    "def error(actual_y, predict_y):\n",
    "    actual_y = actual_y.reshape(5014,)\n",
    "    counter = 0\n",
    "    for i in range(predict_y.shape[0]):\n",
    "        if actual_y[i] != predict_y[i]:\n",
    "            counter += 1\n",
    "    return counter/predict_y.shape[0]\n",
    "\n",
    "train_error = error(Y_train, Y_predicted)\n",
    "train_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
